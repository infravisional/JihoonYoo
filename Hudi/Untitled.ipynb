{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hudi Quick Start with Spark\n",
    "\n",
    "This Jupyter notebook is based on the Quickstart guide on the Apache Hudi official website. Though it is a helpful guide, there were some tweaks that had to be made in order for this notebook to run successfully. \n",
    "\n",
    "Source:\n",
    "https://hudi.apache.org/docs/quick-start-guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook Configure \n",
    "\n",
    "This portion is to configure the EMR notebook to use Hudi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{ \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "          }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Preparation\n",
    "\n",
    "This portion is to prepare the test data. The test data is generated by `sc._jvm.org.apache.hudi.QuickstartUtils.DataGenerator()`, which is provided on the Quickstart guide. \n",
    "\n",
    "Test Data Schema\n",
    "```\n",
    "begin_lat: double\n",
    "begin_lon: double\n",
    "driver: string\n",
    "end_lat: double\n",
    "end_lon: double\n",
    "fare: double\n",
    "partitionpath: string\n",
    "rider: string\n",
    "ts: long\n",
    "uuid: string\n",
    "```\n",
    "\n",
    "Partition Column - The partitionpath column will be used as a partition column. Here are the sample partition values.\n",
    "```\n",
    "americas/united_states/san_francisco\n",
    "americas/brazil/sao_paulo           \n",
    "asia/india/chennai                  \n",
    "```\n",
    "\n",
    "Understanding the partitionpath values, we can assume that the S3 paths will look like this.\n",
    "```\n",
    "s3://bucket-name/americas/united_states/san_francisco/actualData.parquet\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define variables.\n",
    "tableName = \"hudi_trips_cow\"\n",
    "basePath = \"s3://ecs-bi-dev-datalake-raw/hudi_trips_cow\"\n",
    "\n",
    "# basePathWithPartition \n",
    "# Each * represent each folder (aka. prefix in S3) in partition path. \n",
    "# The last * represents the actual parquet files that contain the data.\n",
    "basePathWithPartition = basePath + \"/*/*/*/*\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instatiate DataGenerator and generate 10 new records for inserts.\n",
    "dataGen = sc._jvm.org.apache.hudi.QuickstartUtils.DataGenerator()\n",
    "inserts = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateInserts(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a df based on the data generated above.\n",
    "df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- begin_lat: double (nullable = true)\n",
      " |-- begin_lon: double (nullable = true)\n",
      " |-- driver: string (nullable = true)\n",
      " |-- end_lat: double (nullable = true)\n",
      " |-- end_lon: double (nullable = true)\n",
      " |-- fare: double (nullable = true)\n",
      " |-- partitionpath: string (nullable = true)\n",
      " |-- rider: string (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- uuid: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----------+-------------------+-------------------+------------------+------------------------------------+---------+-------------+------------------------------------+\n",
      "|begin_lat          |begin_lon          |driver    |end_lat            |end_lon            |fare              |partitionpath                       |rider    |ts           |uuid                                |\n",
      "+-------------------+-------------------+----------+-------------------+-------------------+------------------+------------------------------------+---------+-------------+------------------------------------+\n",
      "|0.4726905879569653 |0.46157858450465483|driver-213|0.754803407008858  |0.9671159942018241 |34.158284716382845|americas/brazil/sao_paulo           |rider-213|1640394420617|add08df1-927e-4268-ba67-79f23e7309f1|\n",
      "|0.6100070562136587 |0.8779402295427752 |driver-213|0.3407870505929602 |0.5030798142293655 |43.4923811219014  |americas/brazil/sao_paulo           |rider-213|1640554827638|88ae4c0e-7ebf-4acd-9471-f6ac11cfa8a2|\n",
      "|0.5731835407930634 |0.4923479652912024 |driver-213|0.08988581780930216|0.42520899698713666|64.27696295884016 |americas/united_states/san_francisco|rider-213|1640475055580|93113d38-7a04-40c8-a468-e0534e7a74e7|\n",
      "|0.21624150367601136|0.14285051259466197|driver-213|0.5890949624813784 |0.0966823831927115 |93.56018115236618 |americas/united_states/san_francisco|rider-213|1640682462268|4b9cbb59-97a9-40a3-be5a-408984fda56b|\n",
      "|0.40613510977307   |0.5644092139040959 |driver-213|0.798706304941517  |0.02698359227182834|17.851135255091155|asia/india/chennai                  |rider-213|1640631758009|de6c073d-6f47-4835-baf2-45d179f4f9fe|\n",
      "|0.8742041526408587 |0.7528268153249502 |driver-213|0.9197827128888302 |0.362464770874404  |19.179139106643607|americas/united_states/san_francisco|rider-213|1640665646982|29dbe404-7839-480c-9a1c-b887abfe7132|\n",
      "|0.1856488085068272 |0.9694586417848392 |driver-213|0.38186367037201974|0.25252652214479043|33.92216483948643 |americas/united_states/san_francisco|rider-213|1640710479589|73cbc8a6-b899-4b98-b472-ede6de796274|\n",
      "|0.0750588760043035 |0.03844104444445928|driver-213|0.04376353354538354|0.6346040067610669 |66.62084366450246 |americas/brazil/sao_paulo           |rider-213|1640619454600|7c3bae1f-539d-4714-98c4-8d423c829e82|\n",
      "|0.651058505660742  |0.8192868687714224 |driver-213|0.20714896002914462|0.06224031095826987|41.06290929046368 |asia/india/chennai                  |rider-213|1640699955141|4973bb25-a684-421f-b8ff-13b45bc44af9|\n",
      "|0.11488393157088261|0.6273212202489661 |driver-213|0.7454678537511295 |0.3954939864908973 |27.79478688582596 |americas/united_states/san_francisco|rider-213|1640312891366|47835e33-901c-4f47-b1c9-9b83bcd6a8d6|\n",
      "+-------------------+-------------------+----------+-------------------+-------------------+------------------+------------------------------------+---------+-------------+------------------------------------+"
     ]
    }
   ],
   "source": [
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(partitionpath='americas/united_states/san_francisco'), Row(partitionpath='asia/india/chennai'), Row(partitionpath='americas/brazil/sao_paulo')]"
     ]
    }
   ],
   "source": [
    "# List the distinct partition paths.\n",
    "df.select('partitionpath').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify common DataSourceWriteOptions in the single hudi_options variable.\n",
    "hudi_options = {\n",
    "    'hoodie.table.name': tableName,\n",
    "    'hoodie.datasource.write.recordkey.field': 'uuid',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n",
    "    'hoodie.datasource.write.table.name': tableName,\n",
    "    'hoodie.datasource.write.operation': 'upsert',\n",
    "    'hoodie.datasource.write.precombine.field': 'ts',\n",
    "    'hoodie.upsert.shuffle.parallelism': 2,\n",
    "    'hoodie.insert.shuffle.parallelism': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data into S3.\n",
    "df.write.format(\"hudi\"). \\\n",
    "    options(**hudi_options). \\\n",
    "    mode(\"overwrite\"). \\ # mode(Overwrite) overwrites and recreates the table if it already exists. \n",
    "    save(basePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Data with Redshift Spectrum\n",
    "This is the SQL script that can be run in Redshift to create an exteral table and its partitions for the test dataset. The result of the last SELECT statement should be matching to the initial test data set. \n",
    "\n",
    "```\n",
    "DROP TABLE IF EXISTS datalake.hudi_trips_cow;\n",
    "\n",
    "CREATE EXTERNAL TABLE datalake.hudi_trips_cow \n",
    "    (\n",
    "        begin_lat       Double Precision,\n",
    "        begin_lon       Double Precision,   \n",
    "        driver          varchar(64),   \n",
    "        end_lat         Double Precision,   \n",
    "        end_lon         Double Precision,     \n",
    "        fare            Double Precision,      \n",
    "        rider           varchar(256),  \n",
    "        ts              timestamp,  \n",
    "        uuid            varchar(256)\n",
    "    )\n",
    "PARTITIONED BY(partitionpath varchar(256))\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "STORED AS\n",
    "INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n",
    "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
    "LOCATION 's3://ecs-bi-dev-datalake-raw/hudi_trips_cow';\n",
    "\n",
    "\n",
    "ALTER TABLE datalake.hudi_trips_cow\n",
    "ADD IF NOT EXISTS PARTITION(partitionpath = 'americas/brazil/sao_paulo')\n",
    "LOCATION 's3://ecs-bi-dev-datalake-raw/hudi_trips_cow/americas/brazil/sao_paulo';\n",
    "\n",
    "ALTER TABLE datalake.hudi_trips_cow\n",
    "ADD IF NOT EXISTS PARTITION(partitionpath = 'americas/united_states/san_francisco')\n",
    "LOCATION 's3://ecs-bi-dev-datalake-raw/hudi_trips_cow/americas/united_states/san_francisco';\n",
    "\n",
    "ALTER TABLE datalake.hudi_trips_cow\n",
    "ADD IF NOT EXISTS PARTITION(partitionpath = 'asia/india/chennai')\n",
    "LOCATION 's3://ecs-bi-dev-datalake-raw/hudi_trips_cow/asia/india/chennai';\n",
    "\n",
    "select *\n",
    "from   datalake.hudi_trips_cow;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Data in PySpark\n",
    "\n",
    "Load the data files into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tripsSnapshotDF = spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  load(basePathWithPartition)\n",
    "# load(basePath) use \"/partitionKey=partitionValue\" folder structure for Spark auto partition discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripsSnapshotDF.createOrReplaceTempView(\"hudi_trips_snapshot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select fare, begin_lon, begin_lat, ts from  hudi_trips_snapshot where fare > 20.0\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_trips_snapshot\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel Query\n",
    "\n",
    "Time Travel enables accessing historical data (i.e. data that has been changed or deleted) at any point within a defined period. It serves as a powerful tool for performing the following tasks:\n",
    "\n",
    "- Restoring data-related objects (tables, schemas, and databases) that might have been accidentally or intentionally deleted.\n",
    "- Duplicating and backing up data from key points in the past.\n",
    "- Analyzing data usage/manipulation over specified periods of time.\n",
    "\n",
    "Hudi support time travel query since 0.9.0. Currently three query time formats are supported as given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------------------------+------------------------------------+----------------------------------------------------------------------+-------------------+-------------------+----------+-------------------+-------------------+------------------+------------------------------------+---------+-------------+------------------------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key                  |_hoodie_partition_path              |_hoodie_file_name                                                     |begin_lat          |begin_lon          |driver    |end_lat            |end_lon            |fare              |partitionpath                       |rider    |ts           |uuid                                |\n",
      "+-------------------+--------------------+------------------------------------+------------------------------------+----------------------------------------------------------------------+-------------------+-------------------+----------+-------------------+-------------------+------------------+------------------------------------+---------+-------------+------------------------------------+\n",
      "|20211230183928     |20211230183928_1_3  |2b6e0308-7ce6-458b-8d00-eaf96cc57bea|americas/united_states/san_francisco|09d52856-ba35-4c95-8a8b-c1d64c01b3d0-0_1-24-31_20211230183928.parquet |0.11488393157088261|0.6273212202489661 |driver-213|0.7454678537511295 |0.3954939864908973 |27.79478688582596 |americas/united_states/san_francisco|rider-213|1640481742494|2b6e0308-7ce6-458b-8d00-eaf96cc57bea|\n",
      "|20211230183928     |20211230183928_1_4  |5f5898b6-c6f1-4381-ab31-55ee8726c120|americas/united_states/san_francisco|09d52856-ba35-4c95-8a8b-c1d64c01b3d0-0_1-24-31_20211230183928.parquet |0.1856488085068272 |0.9694586417848392 |driver-213|0.38186367037201974|0.25252652214479043|33.92216483948643 |americas/united_states/san_francisco|rider-213|1640327741320|5f5898b6-c6f1-4381-ab31-55ee8726c120|\n",
      "|20211230183928     |20211230183928_1_7  |cac3792f-25a9-4d0f-9c62-08dfad3c44f6|americas/united_states/san_francisco|09d52856-ba35-4c95-8a8b-c1d64c01b3d0-0_1-24-31_20211230183928.parquet |0.21624150367601136|0.14285051259466197|driver-213|0.5890949624813784 |0.0966823831927115 |93.56018115236618 |americas/united_states/san_francisco|rider-213|1640483940811|cac3792f-25a9-4d0f-9c62-08dfad3c44f6|\n",
      "|20211230183928     |20211230183928_1_8  |c62258ce-2ef5-46a6-976d-c107c5ea0eef|americas/united_states/san_francisco|09d52856-ba35-4c95-8a8b-c1d64c01b3d0-0_1-24-31_20211230183928.parquet |0.8742041526408587 |0.7528268153249502 |driver-213|0.9197827128888302 |0.362464770874404  |19.179139106643607|americas/united_states/san_francisco|rider-213|1640459331177|c62258ce-2ef5-46a6-976d-c107c5ea0eef|\n",
      "|20211230195231     |20211230195231_1_2  |72f981a7-da6e-4fd0-8de4-10399562439e|americas/united_states/san_francisco|09d52856-ba35-4c95-8a8b-c1d64c01b3d0-0_1-79-106_20211230195231.parquet|0.244841817279154  |0.1072756362186601 |driver-243|0.942031609993243  |0.4046750217523756 |15.119997249522644|americas/united_states/san_francisco|rider-243|1640888054620|72f981a7-da6e-4fd0-8de4-10399562439e|\n",
      "|20211230183928     |20211230183928_2_1  |7e5d39d6-f1f7-44dd-9210-55c8fb5e7c00|asia/india/chennai                  |b4a9dcdb-aacf-4cad-887b-a8d3f26619c0-0_2-24-32_20211230183928.parquet |0.40613510977307   |0.5644092139040959 |driver-213|0.798706304941517  |0.02698359227182834|17.851135255091155|asia/india/chennai                  |rider-213|1640724425931|7e5d39d6-f1f7-44dd-9210-55c8fb5e7c00|\n",
      "|20211230183928     |20211230183928_2_5  |cd140c34-9f08-48e5-b783-f8413f081734|asia/india/chennai                  |b4a9dcdb-aacf-4cad-887b-a8d3f26619c0-0_2-24-32_20211230183928.parquet |0.651058505660742  |0.8192868687714224 |driver-213|0.20714896002914462|0.06224031095826987|41.06290929046368 |asia/india/chennai                  |rider-213|1640464038206|cd140c34-9f08-48e5-b783-f8413f081734|\n",
      "|20211230183928     |20211230183928_0_2  |3bbd1d05-e17e-4820-897b-a199035bc81f|americas/brazil/sao_paulo           |187031ef-ab49-4835-8abb-428bae1619a4-0_0-24-30_20211230183928.parquet |0.4726905879569653 |0.46157858450465483|driver-213|0.754803407008858  |0.9671159942018241 |34.158284716382845|americas/brazil/sao_paulo           |rider-213|1640302797410|3bbd1d05-e17e-4820-897b-a199035bc81f|\n",
      "|20211230183928     |20211230183928_0_6  |84af9b63-aec9-41db-8073-5cd02e4edeea|americas/brazil/sao_paulo           |187031ef-ab49-4835-8abb-428bae1619a4-0_0-24-30_20211230183928.parquet |0.6100070562136587 |0.8779402295427752 |driver-213|0.3407870505929602 |0.5030798142293655 |43.4923811219014  |americas/brazil/sao_paulo           |rider-213|1640846486817|84af9b63-aec9-41db-8073-5cd02e4edeea|\n",
      "|20211230195231     |20211230195231_0_1  |682f6b0d-bb63-400d-9d7f-3fa8da11cce5|americas/brazil/sao_paulo           |187031ef-ab49-4835-8abb-428bae1619a4-0_0-79-105_20211230195231.parquet|0.856152038750905  |0.3132477949501916 |driver-243|0.8742438057467156 |0.26923247017036556|2.4995362119815567|americas/brazil/sao_paulo           |rider-243|1640432506750|682f6b0d-bb63-400d-9d7f-3fa8da11cce5|\n",
      "+-------------------+--------------------+------------------------------------+------------------------------------+----------------------------------------------------------------------+-------------------+-------------------+----------+-------------------+-------------------+------------------+------------------------------------+---------+-------------+------------------------------------+"
     ]
    }
   ],
   "source": [
    "timeTravelDf = spark.read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  option(\"as.of.instant\", \"20210728141108\"). \\\n",
    "  load(basePathWithPartition)\n",
    "\n",
    "\"\"\"\n",
    "spark.read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  option(\"as.of.instant\", \"2021-07-28 14: 11: 08\"). \\\n",
    "  load(basePath)\n",
    "\n",
    "# It is equal to \"as.of.instant = 2021-07-28 00:00:00\"\n",
    "spark.read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  option(\"as.of.instant\", \"2021-07-28\"). \\\n",
    "  load(basePath)\n",
    "\"\"\"\n",
    "\n",
    "timeTravelDf.show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Data\n",
    "\n",
    "This is similar to inserting new data. Generate updates to existing trips using the data generator, load into a DataFrame and write DataFrame into the hudi table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate 5 update records using QuickstartUtils.\n",
    "updates = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateUpdates(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.json(spark.sparkContext.parallelize(updates, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- begin_lat: double (nullable = true)\n",
      " |-- begin_lon: double (nullable = true)\n",
      " |-- driver: string (nullable = true)\n",
      " |-- end_lat: double (nullable = true)\n",
      " |-- end_lon: double (nullable = true)\n",
      " |-- fare: double (nullable = true)\n",
      " |-- partitionpath: string (nullable = true)\n",
      " |-- rider: string (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- uuid: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+----------+------------------+-------------------+------------------+------------------------------------+---------+-------------+------------------------------------+\n",
      "|begin_lat          |begin_lon           |driver    |end_lat           |end_lon            |fare              |partitionpath                       |rider    |ts           |uuid                                |\n",
      "+-------------------+--------------------+----------+------------------+-------------------+------------------+------------------------------------+---------+-------------+------------------------------------+\n",
      "|0.7340133901254792 |0.5142184937933181  |driver-284|0.7814655558162802|0.6592596683641996 |49.527694252432056|americas/united_states/san_francisco|rider-284|1640773277931|93113d38-7a04-40c8-a468-e0534e7a74e7|\n",
      "|0.1593867607188556 |0.010872312870502165|driver-284|0.9808530350038475|0.7963756520507014 |29.47661370147079 |americas/brazil/sao_paulo           |rider-284|1640493172214|add08df1-927e-4268-ba67-79f23e7309f1|\n",
      "|0.7180196467760873 |0.13755354862499358 |driver-284|0.3037264771699937|0.2539047155055727 |86.75932789048282 |americas/brazil/sao_paulo           |rider-284|1640829595043|add08df1-927e-4268-ba67-79f23e7309f1|\n",
      "|0.6570857443423376 |0.888493603696927   |driver-284|0.9036309069576131|0.37603706507284995|63.72504913279929 |americas/brazil/sao_paulo           |rider-284|1640881999594|7c3bae1f-539d-4714-98c4-8d423c829e82|\n",
      "|0.08528650347654165|0.4006983139989222  |driver-284|0.1975324518739051|0.908216792146506  |90.25710109008239 |asia/india/chennai                  |rider-284|1640592065829|de6c073d-6f47-4835-baf2-45d179f4f9fe|\n",
      "+-------------------+--------------------+----------+------------------+-------------------+------------------+------------------------------------+---------+-------------+------------------------------------+"
     ]
    }
   ],
   "source": [
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Notice here that the mode is append, not overwrite.\n",
    "df.write.format(\"hudi\"). \\\n",
    "  options(**hudi_options). \\\n",
    "  mode(\"append\"). \\\n",
    "  save(basePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the update is executed successfully, check the data set again either using the Redshift Spectrum table or using the PySpark method and see if the data gets updated correctly. Check based on the uuid column (record key). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental Query\n",
    "\n",
    "Hudi also provides capability to obtain a stream of records that changed since given commit timestamp. This can be achieved using Hudi's incremental querying and providing a begin time from which changes need to be streamed. We do not need to specify endTime, if we want all changes after the given commit (as is the common case). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reload data\n",
    "spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  load(basePathWithPartition). \\\n",
    "  createOrReplaceTempView(\"hudi_trips_snapshot\")\n",
    "\n",
    "commits = list(map(lambda row: row[0], spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime\").limit(50).collect()))\n",
    "beginTime = commits[len(commits) - 2] # commit time we are interested in\n",
    "\n",
    "# incrementally query data\n",
    "incremental_read_options = {\n",
    "  'hoodie.datasource.query.type': 'incremental',\n",
    "  'hoodie.datasource.read.begin.instanttime': beginTime,\n",
    "}\n",
    "\n",
    "tripsIncrementalDF = spark.read.format(\"hudi\"). \\\n",
    "  options(**incremental_read_options). \\\n",
    "  load(basePathWithPartition)\n",
    "tripsIncrementalDF.createOrReplaceTempView(\"hudi_trips_incremental\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+-------------------+-------------+\n",
      "|_hoodie_commit_time|              fare|          begin_lon|          begin_lat|           ts|\n",
      "+-------------------+------------------+-------------------+-------------------+-------------+\n",
      "|     20211230202633|49.527694252432056| 0.5142184937933181| 0.7340133901254792|1640773277931|\n",
      "|     20211230202633| 63.72504913279929|  0.888493603696927| 0.6570857443423376|1640881999594|\n",
      "|     20211230202633| 86.75932789048282|0.13755354862499358| 0.7180196467760873|1640829595043|\n",
      "|     20211230202633| 90.25710109008239| 0.4006983139989222|0.08528650347654165|1640592065829|\n",
      "+-------------------+------------------+-------------------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare > 20.0\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete data\n",
    "\n",
    "You can delete records as shown below. Also fyi - to hard delete a record, you can upsert an empty payload. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14"
     ]
    }
   ],
   "source": [
    "# Before Delete Count\n",
    "spark.sql(\"select uuid, partitionpath from hudi_trips_snapshot\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fetch two records to be deleted\n",
    "ds = spark.sql(\"select uuid, partitionpath from hudi_trips_snapshot\").limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Can also create a separate hudi config variable like this.\n",
    "hudi_delete_options = {\n",
    "  'hoodie.table.name': tableName,\n",
    "  'hoodie.datasource.write.recordkey.field': 'uuid',\n",
    "  'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n",
    "  'hoodie.datasource.write.table.name': tableName,\n",
    "  'hoodie.datasource.write.operation': 'delete',\n",
    "  'hoodie.datasource.write.precombine.field': 'ts',\n",
    "  'hoodie.upsert.shuffle.parallelism': 2, \n",
    "  'hoodie.insert.shuffle.parallelism': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deletes = list(map(lambda row: (row[0], row[1]), ds.collect()))\n",
    "df = spark.sparkContext.parallelize(deletes).toDF(['uuid', 'partitionpath']).withColumn('ts', lit(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------------------------+---+\n",
      "|uuid                                |partitionpath                       |ts |\n",
      "+------------------------------------+------------------------------------+---+\n",
      "|2b6e0308-7ce6-458b-8d00-eaf96cc57bea|americas/united_states/san_francisco|0.0|\n",
      "|5f5898b6-c6f1-4381-ab31-55ee8726c120|americas/united_states/san_francisco|0.0|\n",
      "+------------------------------------+------------------------------------+---+"
     ]
    }
   ],
   "source": [
    "df.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.format(\"hudi\"). \\\n",
    "  options(**hudi_delete_options). \\\n",
    "  mode(\"append\"). \\\n",
    "  save(basePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12"
     ]
    }
   ],
   "source": [
    "# run the same read query as above.\n",
    "roAfterDeleteViewDF = spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  load(basePathWithPartition) \n",
    "roAfterDeleteViewDF.registerTempTable(\"hudi_trips_snapshot\")\n",
    "# fetch should return (total - 2) records\n",
    "spark.sql(\"select uuid, partitionpath from hudi_trips_snapshot\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
